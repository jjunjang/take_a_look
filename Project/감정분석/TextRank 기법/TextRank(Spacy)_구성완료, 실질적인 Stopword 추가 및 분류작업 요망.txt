import time

from collections import OrderedDict
import numpy as np
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.tokenizer import Tokenizer


import csv
import re
from collections import Counter
from nltk.corpus import stopwords

start = time.time()  # 시작 시간 저장
nlp = spacy.load('en_core_web_md')

nlp.max_length = 93621305  # nlp 메모리 용량 한도를 높여주는 작업
stop_words = set(stopwords.words('english'))  # 불용어 제거용 stopwords
class TextInput():
    # 어떤 리뷰파일을 읽을것인지 고유 파일명을 추출하는 단.
    def Tconst_List():
        t_const = list()
        with open('C:\\Users\\Cyphe\\Desktop\\python\\Movie_Tconst.csv') as f:
            for i in f:
                i = i.replace('\n', '')
                t_const.append(i)

        return t_const

    # 리뷰 파일을 읽어와서 저장하는 단.
    def Input():
        global Star, Id, Title, Content
        t_const = 'tt1179904'
            #TextInput.Tconst_List()

        f = open("D:\\Project\\WebCrowling\\" + t_const + ".tsv", 'r', encoding='utf-8')
        rdr = list(csv.reader(f, delimiter='\t', quoting=3))

        Star, Id, Title, Content = [], [], [], []
        f.close()

        score = 0
        # 읽어온 파일을 추가하는 단.
        for line in rdr:
            Star.append(line[0])
            Id.append(line[1])
            Title.append(line[2])
            Content.append(line[3])
            score += 1
            if score > 2000:
                break
        return Star, Id, Title, Content


    # 읽어온 Title,Content를 토큰화 / 정제화 뒤, 인덱스화 하는단.
    def text_refine():
        Star, Id, Title, Content = TextInput.Input()
        Title_Result, Content_Result, Difinitive_Title, Difinitive_Content = [], [], [], []
        #vocab_Title, vocab_Content = Counter() , Counter() # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.

        for word in Title:
            word = word.lower()  # 소문자
            Difinitive_Title.append(word)  # 모든 정제작업이 이루워진 배열이 Difinitive 배열

        for word in Content:
            word = word.lower()  # 소문자
            Difinitive_Content.append(word)

        #global T_vocab_sorted, C_vocab_sorted
        #T_vocab_sorted = sorted(vocab_Title.items(), key=lambda x: x[1], reverse=True)
        #C_vocab_sorted = sorted(vocab_Content.items(), key=lambda x: x[1], reverse=True)

        return Difinitive_Title, Difinitive_Content


    #영화별 장르 갯수를 맞춰주는 단
    def Input_Genres():
        with open('C:\\Users\\Cyphe\\Desktop\\python\\Movie_Genres.csv') as f:
            genres_count = list()
            for line in f:
                genres_count.append(line.replace("\"", "").replace("\n", "").split(","))

        for i in range(len(genres_count)):
            if genres_count[i].__len__() < 4:
                while (True):
                    genres_count[i].append("0")
                    if genres_count[i].__len__() == 4:
                        break
        return genres_count





#실제 분류하는 단.
class Classification():
    #Spacy Unsupervised learning 방법에 의거, 뽑혀나온 키워드를 기준으로 영화의 직접적인 평가를 결정 짓습니다.
    def classification(self, key, value):
        self.key = key
        self.value = value
        genres_value = TextInput.Input_Genres()

        if key == "train" and value >= 3.0:
            return "설국열차"





class TextRank4Keyword():
    """텍스트에서 키워드를 추출합니다."""

    def __init__(self):
        self.d = 0.85  # 감쇠 계수로, 일반적으로 0.85를 추천합니다/
        self.min_diff = 1e-5  # 수렴 임계값
        self.steps = 10  # 반복단계
        self.node_weight = None  # save keywords and its weight

    def set_stopwords(self, stopwords):
        nlp.Defaults.stop_words |= {"movie", "film", "review", "plot", "expect", "spoiler",
                                    "scene", "watch", "time", "age", "seen"
                                    "way", "character"}
        for word in STOP_WORDS.union(set(stopwords)):
            lexeme = nlp.vocab[word]
            lexeme.is_stop = True


    def sentence_segment(self, doc, candidate_pos, lower):
        """해당 단어를 cadidate_pos 로만 저장하십시오."""
        sentences = []
        for sent in doc.sents:
            selected_words = []
            sent = nlp(sent.lemma_)
            for token in sent:
                # 후보 POS 태그로만 단어를 저장합니다.
                if str(token.pos_) in candidate_pos and token.is_stop is False:
                    selected_words.append(token.text)

            sentences.append(selected_words)
        return sentences


    def get_vocab(self, sentences):
        """모든 토큰을 가져옵니다."""
        vocab = OrderedDict()
        i = 0
        for sentence in sentences:
            for word in sentence:
                if word not in vocab:
                    vocab[word] = i
                    i += 1
        return vocab


    def get_token_pairs(self, window_size, sentences):
        """창 안 문장에서 token_pairs를 빌드합니다."""
        token_pairs = list()
        for sentence in sentences:
            for i, word in enumerate(sentence):
                for j in range(i + 1, i + window_size):
                    if j >= len(sentence):
                        break
                    pair = (word, sentence[j])
                    if pair not in token_pairs:
                        token_pairs.append(pair)
        return token_pairs


    def symmetrize(self, a):
        return a + a.T - np.diag(a.diagonal())


    def get_matrix(self, vocab, token_pairs):
        """정규화된 행렬을 만듭니다."""
        # Build matrix
        vocab_size = len(vocab)
        g = np.zeros((vocab_size, vocab_size), dtype='float')
        for word1, word2 in token_pairs:
            i, j = vocab[word1], vocab[word2]
            g[i][j] = 1

        # Get Symmeric matrix
        g = self.symmetrize(g)

        # Normalize matrix by column
        norm = np.sum(g, axis=0)
        g_norm = np.divide(g, norm, where=norm != 0)  # norm norm이 0이면 이를 무시합니다.
        #print(g[:3][:5])
        #print(norm[:3])
        #print(g_norm[:3][:5])
        return g_norm


    def get_keywords(self, number=10):
        """최상위 키워드를 인쇄합니다."""
        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))
        classification_class = Classification()
        for i, (key, value) in enumerate(node_weight.items()):
            K = classification_class.classification(key, value)
            print(key + ' - ' + str(value))
            if i > number:
                break

    def analyze(self, text,
                candidate_pos=['NOUN', 'VERB'],
                window_size=4, lower=False, stopwords=list()):
        """텍스트를 분석하는 메인 함수."""

        # 스탑 워드를 설정합니다.
        self.set_stopwords(stopwords)

        # Spacy 알고리즘을 통해 텍스트를 다듬습니다!
        doc = nlp(text)

        # 문장을 필터링 합니다.
        sentences = self.sentence_segment(doc, candidate_pos, lower)  # sentences는 이제 온전한 단어의 목록을 나타냅니다.

        # 단어별 백터화를 통해 번호를 매기는 작업입니다.
        vocab = self.get_vocab(sentences)

        # windows 에서 토큰 페어를 계산 후, 가져옵니다.
        token_pairs = self.get_token_pairs(window_size, sentences)

        # 정규화된 행렬을 가져옵니다.
        g = self.get_matrix(vocab, token_pairs)

        # 가중치를 초기화 합니다 (Page Rank 값) [1로 이루워진 행렬입니다]
        pr = np.array([1] * len(vocab))

        # 반복합니다
        previous_pr = 0
        for epoch in range(self.steps):
            pr = (1 - self.d) + self.d * np.dot(g, pr)
            if abs(previous_pr - sum(pr)) < self.min_diff:
                break
            else:
                previous_pr = sum(pr)

        # 각 노드에 대한 가중치를 가져옵니다.
        node_weight = dict()
        for word, index in vocab.items():
            node_weight[word] = pr[index]

        self.node_weight = node_weight



title, content = TextInput.text_refine()
text = str(title)
tr4w = TextRank4Keyword()
tr4w.analyze(text, candidate_pos = ['NOUN', 'VERB'], window_size=4, lower=False)
tr4w.get_keywords(200)
print("time :", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간
