import csv
import re
import nltk
from nltk.tokenize import TreebankWordTokenizer
from nltk.corpus import stopwords
from collections import Counter
from multiprocessing import Pool
# shortword = re.compile(r'\W*\b\w{1,2}\b')

vocab_Title=Counter()      # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.
vocab_Content = Counter()

tokenizer = TreebankWordTokenizer()         #토큰화를 위해
Difinitive_Title = []                       #최종 저장될 Title
Difinitive_Content = []                     #최종 저장될 Content
Title_to_Index = {}                         #토큰화된 타이틀 인덱스
Content_to_Index = {}                       #토큰화된 컨텐츠 인덱스

stemmer = nltk.stem.PorterStemmer()         #PoterStemmer 를 이용하여 스테밍
stop_words = set(stopwords.words('english'))#불용어 제거용 stopwords


# 불용어 추가하는 단
def add_stopword():
    global stop_words
    stop_words = nltk.corpus.stopwords.words('english')
    newstopwords = ['movi', 'film', 'one', 'two','imdb','cinema']
    stop_words.extend(newstopwords)

def Input():
    #파일 불러와 읽는단
    f = open("D:\\Project\\WebCrowling\\tt0387564.tsv", 'r', encoding='utf-8')
    rdr = list(csv.reader(f, delimiter='\t', quoting=3))
    global Star, Id, Title, Content
    Star = []
    Id = []
    Title = []
    Content = []
    f.close()

    #읽어온 파일을 추가하는 단.
    for line in rdr:
        Star.append(line[0])
        Id.append(line[1])
        Title.append(line[2])
        Content.append(line[3])

# 읽어온 Title,Content를 토큰화 / 정제하는 단
def text_refine():
    for i in range(len(Content)):
        Title_Token = tokenizer.tokenize(Title[i])     #타이틀 토큰화
        Content_Token = tokenizer.tokenize(Content[i]) #컨텐츠 토큰화
        Title_Result = []
        Content_Result = []

        for word in Title_Token:
            word = re.sub('[^a-zA-Z]', '', word)          # 영문자 제외 모두 삭제
            word = word.lower()                          # 소문자
            if word not in stop_words:                  # 불용어에 포함되어 있지 않은것
                if len(word) > 2:                       # 길이가 2자리 이하인것은 제외
                    word = format(stemmer.stem(word))
                    Title_Result.append(word)           # 을 추가.
                    vocab_Title[word] = vocab_Title[word] + 1
        Difinitive_Title.append(Title_Result)          # 모든 정제작업이 이루워진 배열이 Difinitive 배열

        for word in Content_Token:
            word = re.sub('[^a-zA-Z]', '', word)        # 영문자 제외 모두 삭제
            word = word.lower()                        # 소문자
            if word not in stop_words:                # 불용어에 포함되어 있지 않은것
                if len(word) > 2:                     # 길이가 2자리 이하인것은 제외
                    word = format(stemmer.stem(word))
                    Content_Result.append(word)       # 을 추가.
                    vocab_Content[word] = vocab_Content[word] + 1
        Difinitive_Content.append(Content_Result)
   
    global T_vocab_sorted, C_vocab_sorted
    T_vocab_sorted = sorted(vocab_Title.items(), key=lambda x:x[1], reverse=True)
    C_vocab_sorted = sorted(vocab_Content.items(), key=lambda x:x[1], reverse=True)

def after_sorted():
    i = 0
    k = 0
    global Title_to_Index, Content_to_Index

    #토큰화된 단어별 인덱스를 정렬하는 단.
    for(word, frequency) in T_vocab_sorted :
        if frequency > 4 :
            i += 1
            Title_to_Index[word] = i
    for(word, frequency) in C_vocab_sorted :
        if frequency > 4 :
            k += 1
            Content_to_Index[word] = k

add_stopword()
Input()
text_refine()
after_sorted()

print(C_vocab_sorted)





____________________________



import logging
logging.basicConfig(
    format='%(asctime)s : %(levelname)s : %(message)s', 
    level=logging.INFO)


# 파라메터값 지정
num_features = 300 # 문자 벡터 차원 수
min_word_count = 40 # 최소 문자 수
num_workers = 16 # 병렬 처리 스레드 수
context = 10 # 문자열 창 크기
downsampling = 1e-3 # 문자 빈도 수 Downsample

# 초기화 및 모델 학습
from gensim.models import FastText

# 모델 학습
model = FastText(Difinitive_Content, 
                          workers=num_workers, 
                          size=num_features, 
                          min_count=min_word_count,
                          window=context,
                          sample=downsampling)

model



______________________________






# 학습이 완료 되면 필요없는 메모리를 unload 시킨다.
model.init_sims(replace=True)

model_name = '300features_40minwords_10text'

# model_name = '300features_50minwords_20text'
model.save(model_name)

print(model)
print(Difinitive_Content)




______________________________



model.wv.most_similar("cut") # stemming 처리 시


model.wv.doesnt_match('villen'.split())