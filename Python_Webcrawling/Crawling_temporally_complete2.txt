#셀레니움, 뷰티풀스프 검색해서 pip install 하면 됨 (cmd에서)
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
import csv
import time

#최종 출력 변수
Star = []
Title = []
Content = []

# chromedriver 사용.
driver = webdriver.Chrome('D:\chromedriver.exe')
driver.get('https://www.imdb.com/title/tt4154796/reviews?ref_=tt_ov_rt')

# 바디 영역 저장
body = driver.find_element_by_tag_name("body")

def Click():
    while(True):
        try:
            time.sleep(3)
            driver.find_element_by_xpath("//*[@id='load-more-trigger']").click()
        except:
            break;

    return driver

# Neither self-contained nor original

def Crawling(driver):
    # BeautifulSoup 사용한다. 특히 html란은 여기 선언해 줘야지 이미 다 불러온 page_source를 불러온다.
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")
    reviews = soup.find_all('div', class_='review-container')

    for review in reviews:
        rating = review.find('span', class_='rating-other-user-rating')
        if rating:
            rating = ''.join(i.text for i in rating.find_all('span'))
        rating = rating if rating else '0'
        Star.append(rating)

    for title in soup.find_all('a', {'class': 'title'}):
        Title.append(title.text.strip())

    for content in soup.find_all(True,{'class' :['text show-more__control','text show-more__control clickable']}):
        Content.append(content.text.strip())

def Gabage_Remove(inputArray):
    Gabage_Result = []
    for i in range(len(inputArray)):
        A = str(inputArray[i])
        A = A.replace('**SPOILER ALERT**', '')
        A = A.replace('**Spoilers alert**', '')
        A = A.replace('\n','')
        Gabage_Result.append(A)
    return Gabage_Result

def Gabage_Remove_Star(inputArray):
    Gabage_Result = []
    for i in range(len(inputArray)):
        A = str(inputArray[i])
        A = A.replace("/10",'')
        Gabage_Result.append(A)
    return Gabage_Remove()



Crawling(Click())
Star_R = Gabage_Remove_Star(Star)
Title_R = Gabage_Remove(Title)
Content_R = Gabage_Remove(Content)

with open("D:\Project\WebCrowling\\tt4154797.tsv", 'w', encoding='utf-8') as f:
    for row in range(len(Star)):
        f.write(Star_R[row])
        f.write("\t")
        f.write(Title_R[row])
        f.write("\t")
        f.write(Content_R[row])
        f.write("\n")



print(range(len(Star)))
print(range(len(Title)))
print(range(len(Content)))

#한계점 , 현재 이걸 다 돌릴려면 약 50초가량 걸리는데, 콜백계념을 못써서 while에서 하루종일 주구장창 돌기때문에 그럼...
#때문에 개선할 필요성이 있다.